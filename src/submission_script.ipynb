{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from config import model_name\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import path\n",
    "import sys\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import importlib\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# model_name: str = 'NRMS'\n",
    "# model_name: str = 'NAML'\n",
    "# model_name: str = 'TANR'\n",
    "# model_name: str = 'LSTUR'\n",
    "# model_name: str = 'DKN'\n",
    "# model_name: str = 'HiFiArk'\n",
    "# model_name: str = 'Exp1'\n",
    "\n",
    "try:\n",
    "    Model = getattr(importlib.import_module(f\"model.{model_name}\"), model_name)\n",
    "    config = getattr(importlib.import_module('config'), f\"{model_name}Config\")\n",
    "except AttributeError:\n",
    "    print(f\"{model_name} not included!\")\n",
    "    exit()\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RESULT_CSV = 'results.csv'\n",
    "norm = lambda x: (x-np.min(x)) / (np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2**y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n",
    "\n",
    "\n",
    "def value2rank(d):\n",
    "    values = list(d.values())\n",
    "    ranks = [sorted(values, reverse=True).index(x) for x in values]\n",
    "    return {k: ranks[i] + 1 for i, k in enumerate(d.keys())}\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load news for evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, news_path):\n",
    "        super(NewsDataset, self).__init__()\n",
    "        self.news_parsed = pd.read_table(\n",
    "            news_path,\n",
    "            usecols=['id'] + config.dataset_attributes['news'],\n",
    "            converters={\n",
    "                attribute: literal_eval\n",
    "                for attribute in set(config.dataset_attributes['news']) & set([\n",
    "                    'title', 'abstract', 'title_entities', 'abstract_entities'\n",
    "                ])\n",
    "            })\n",
    "        self.news2dict = self.news_parsed.to_dict('index')\n",
    "        for key1 in self.news2dict.keys():\n",
    "            for key2 in self.news2dict[key1].keys():\n",
    "                if type(self.news2dict[key1][key2]) != str:\n",
    "                    self.news2dict[key1][key2] = torch.tensor(\n",
    "                        self.news2dict[key1][key2])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.news_parsed)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.news2dict[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "class UserDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load users for evaluation, duplicated rows will be dropped\n",
    "    \"\"\"\n",
    "    def __init__(self, behaviors_path, user2int_path):\n",
    "        super(UserDataset, self).__init__()\n",
    "        self.behaviors = pd.read_table(behaviors_path,\n",
    "                                       header=None,\n",
    "                                       usecols=[1, 3],\n",
    "                                       names=['user', 'clicked_news'])\n",
    "        self.behaviors.clicked_news.fillna(' ', inplace=True)\n",
    "        self.behaviors.drop_duplicates(inplace=True)\n",
    "        user2int = dict(pd.read_table(user2int_path).values.tolist())\n",
    "        user_total = 0\n",
    "        user_missed = 0\n",
    "        for row in self.behaviors.itertuples():\n",
    "            user_total += 1\n",
    "            if row.user in user2int:\n",
    "                self.behaviors.at[row.Index, 'user'] = user2int[row.user]\n",
    "            else:\n",
    "                user_missed += 1\n",
    "                self.behaviors.at[row.Index, 'user'] = 0\n",
    "        if model_name == 'LSTUR':\n",
    "            print(f'User miss rate: {user_missed/user_total:.4f}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.behaviors.iloc[idx]\n",
    "        item = {\n",
    "            \"user\":\n",
    "            row.user,\n",
    "            \"clicked_news_string\":\n",
    "            row.clicked_news,\n",
    "            \"clicked_news\":\n",
    "            row.clicked_news.split()[:config.num_clicked_news_a_user]\n",
    "        }\n",
    "        item['clicked_news_length'] = len(item[\"clicked_news\"])\n",
    "        repeated_times = config.num_clicked_news_a_user - len(\n",
    "            item[\"clicked_news\"])\n",
    "        assert repeated_times >= 0\n",
    "        item[\"clicked_news\"] = ['PADDED_NEWS'\n",
    "                                ] * repeated_times + item[\"clicked_news\"]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "class BehaviorsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load behaviors for evaluation, (user, time) pair as session\n",
    "    \"\"\"\n",
    "    def __init__(self, behaviors_path):\n",
    "        super(BehaviorsDataset, self).__init__()\n",
    "        self.behaviors = pd.read_table(behaviors_path,\n",
    "                                       header=None,\n",
    "                                       usecols=range(5),\n",
    "                                       names=[\n",
    "                                           'impression_id', 'user', 'time',\n",
    "                                           'clicked_news', 'impressions'\n",
    "                                       ])\n",
    "        self.behaviors.clicked_news.fillna(' ', inplace=True)\n",
    "        self.behaviors.impressions = self.behaviors.impressions.str.split()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.behaviors.iloc[idx]\n",
    "        item = {\n",
    "            \"impression_id\": row.impression_id,\n",
    "            \"user\": row.user,\n",
    "            \"time\": row.time,\n",
    "            \"clicked_news_string\": row.clicked_news,\n",
    "            \"impressions\": row.impressions\n",
    "        }\n",
    "        return item\n",
    "\n",
    "\n",
    "def calculate_single_user_metric(pair):\n",
    "    try:\n",
    "        auc = roc_auc_score(*pair)\n",
    "        mrr = mrr_score(*pair)\n",
    "        ndcg5 = ndcg_score(*pair, 5)\n",
    "        ndcg10 = ndcg_score(*pair, 10)\n",
    "        return [auc, mrr, ndcg5, ndcg10]\n",
    "    except ValueError:\n",
    "        return [np.nan] * 4\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, directory, num_workers, max_count=sys.maxsize, mode='test'):\n",
    "    \"\"\"\n",
    "    Evaluate model on target directory.\n",
    "    Args:\n",
    "        model: model to be evaluated\n",
    "        directory: the directory that contains two files (behaviors.tsv, news_parsed.tsv)\n",
    "        num_workers: processes number for calculating metrics\n",
    "    Returns:\n",
    "        AUC\n",
    "        MRR\n",
    "        nDCG@5\n",
    "        nDCG@10\n",
    "    \"\"\"\n",
    "    news_dataset = NewsDataset(path.join(directory, 'news_parsed.tsv'))\n",
    "    news_dataloader = DataLoader(news_dataset,\n",
    "                                 batch_size=config.batch_size * 16,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=config.num_workers,\n",
    "                                 drop_last=False,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    news2vector = {}\n",
    "    for minibatch in tqdm(news_dataloader,\n",
    "                          desc=\"Calculating vectors for news\"):\n",
    "        news_ids = minibatch[\"id\"]\n",
    "        if any(id not in news2vector for id in news_ids):\n",
    "            news_vector = model.get_news_vector(minibatch)\n",
    "            for id, vector in zip(news_ids, news_vector):\n",
    "                if id not in news2vector:\n",
    "                    news2vector[id] = vector\n",
    "\n",
    "    news2vector['PADDED_NEWS'] = torch.zeros(\n",
    "        list(news2vector.values())[0].size())\n",
    "\n",
    "    user_dataset = UserDataset(path.join(directory, 'behaviors.tsv'),\n",
    "                               'data/train/user2int.tsv')\n",
    "    user_dataloader = DataLoader(user_dataset,\n",
    "                                 batch_size=config.batch_size * 16,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=config.num_workers,\n",
    "                                 drop_last=False,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    user2vector = {}\n",
    "    for minibatch in tqdm(user_dataloader,\n",
    "                          desc=\"Calculating vectors for users\"):\n",
    "        user_strings = minibatch[\"clicked_news_string\"]\n",
    "        if any(user_string not in user2vector for user_string in user_strings):\n",
    "            clicked_news_vector = torch.stack([\n",
    "                torch.stack([news2vector[x].to(device) for x in news_list],\n",
    "                            dim=0) for news_list in minibatch[\"clicked_news\"]\n",
    "            ],\n",
    "                                              dim=0).transpose(0, 1)\n",
    "            if model_name == 'LSTUR':\n",
    "                user_vector = model.get_user_vector(\n",
    "                    minibatch['user'], minibatch['clicked_news_length'],\n",
    "                    clicked_news_vector)\n",
    "            else:\n",
    "                user_vector = model.get_user_vector(clicked_news_vector)\n",
    "            for user, vector in zip(user_strings, user_vector):\n",
    "                if user not in user2vector:\n",
    "                    user2vector[user] = vector\n",
    "\n",
    "    behaviors_dataset = BehaviorsDataset(path.join(directory, 'behaviors.tsv'))\n",
    "    behaviors_dataloader = DataLoader(behaviors_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=config.num_workers)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    tasks = []\n",
    "    result_dict = {}\n",
    "\n",
    "    for minibatch in tqdm(behaviors_dataloader,\n",
    "                          desc=\"Calculating probabilities\"):\n",
    "        count += 1\n",
    "        if count == max_count:\n",
    "            break\n",
    "\n",
    "        candidate_news_vector = torch.stack([\n",
    "            news2vector[news[0].split('-')[0]]\n",
    "            for news in minibatch['impressions']\n",
    "        ],\n",
    "                                            dim=0)\n",
    "        user_vector = user2vector[minibatch['clicked_news_string'][0]]\n",
    "        click_probability = model.get_prediction(candidate_news_vector,\n",
    "                                                 user_vector)\n",
    "\n",
    "        y_pred = click_probability.tolist()\n",
    "        if mode == 'train':\n",
    "            y_true = [\n",
    "                int(news[0].split('-')[1]) for news in minibatch['impressions']\n",
    "            ]\n",
    "            tasks.append((y_true, y_pred))\n",
    "        elif mode == 'test':\n",
    "            result_dict[f'{count-1}'] = norm(y_pred)\n",
    "\n",
    "    if mode == 'train':\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            results = pool.map(calculate_single_user_metric, tasks)\n",
    "\n",
    "        aucs, mrrs, ndcg5s, ndcg10s = np.array(results).T\n",
    "        return np.nanmean(aucs), np.nanmean(mrrs), np.nanmean(ndcg5s), np.nanmean(\n",
    "            ndcg10s)\n",
    "    elif mode == 'test':\n",
    "        return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n",
      "Evaluating model TANR\n",
      "Load saved parameters in ./checkpoint/TANR/ckpt-8000.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb935d758cac4ee3bced3515037cf6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating vectors for news:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e8839d715d42648a923394c89c4a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating vectors for users:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399722fca7694d69adfc7622023e5602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities:   0%|          | 0/28531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7373\n",
      "MRR: 0.4098\n",
      "nDCG@5: 0.4903\n",
      "nDCG@10: 0.5820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb0f765b9d0459190f2c351c716ea1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating vectors for news:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd63a0d79bd84251b1f948ed949c06c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating vectors for users:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90805cc79ac3480dbc3f7b943b932f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities:   0%|          | 0/46332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Using device:', device)\n",
    "print(f'Evaluating model {model_name}')\n",
    "# Don't need to load pretrained word/entity/context embedding\n",
    "# since it will be loaded from checkpoint later\n",
    "model = Model(config).to(device)\n",
    "from train import latest_checkpoint  # Avoid circular imports\n",
    "checkpoint_path = latest_checkpoint(path.join('./checkpoint', model_name))\n",
    "if checkpoint_path is None:\n",
    "    print('No checkpoint file found!')\n",
    "    exit()\n",
    "print(f\"Load saved parameters in {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "auc, mrr, ndcg5, ndcg10 = evaluate(model, './data/val',\n",
    "                                   config.num_workers, mode='train')\n",
    "print(\n",
    "    f'AUC: {auc:.4f}\\nMRR: {mrr:.4f}\\nnDCG@5: {ndcg5:.4f}\\nnDCG@10: {ndcg10:.4f}'\n",
    ")\n",
    "\n",
    "y_preds = evaluate(model, './data/test', config.num_workers, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>p14</th>\n",
       "      <th>p15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.965313</td>\n",
       "      <td>0.204751</td>\n",
       "      <td>0.035420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124784</td>\n",
       "      <td>0.151610</td>\n",
       "      <td>0.136660</td>\n",
       "      <td>0.193609</td>\n",
       "      <td>0.398131</td>\n",
       "      <td>0.265642</td>\n",
       "      <td>0.128594</td>\n",
       "      <td>0.372625</td>\n",
       "      <td>0.641762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.556415</td>\n",
       "      <td>0.373567</td>\n",
       "      <td>0.358529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270856</td>\n",
       "      <td>0.229947</td>\n",
       "      <td>0.308473</td>\n",
       "      <td>0.762070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600530</td>\n",
       "      <td>0.113563</td>\n",
       "      <td>0.383396</td>\n",
       "      <td>0.488201</td>\n",
       "      <td>0.440063</td>\n",
       "      <td>0.491090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.752334</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607291</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.338792</td>\n",
       "      <td>0.193911</td>\n",
       "      <td>0.555606</td>\n",
       "      <td>0.072123</td>\n",
       "      <td>0.530488</td>\n",
       "      <td>0.739669</td>\n",
       "      <td>0.158514</td>\n",
       "      <td>0.474470</td>\n",
       "      <td>0.496779</td>\n",
       "      <td>0.268669</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.494238</td>\n",
       "      <td>0.235841</td>\n",
       "      <td>0.402977</td>\n",
       "      <td>0.228699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.438163</td>\n",
       "      <td>0.311420</td>\n",
       "      <td>0.381508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377915</td>\n",
       "      <td>0.207145</td>\n",
       "      <td>0.268098</td>\n",
       "      <td>0.249367</td>\n",
       "      <td>0.330327</td>\n",
       "      <td>0.658996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.054635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.309762</td>\n",
       "      <td>0.682028</td>\n",
       "      <td>0.552010</td>\n",
       "      <td>0.589548</td>\n",
       "      <td>0.633458</td>\n",
       "      <td>0.907023</td>\n",
       "      <td>0.952393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071843</td>\n",
       "      <td>0.415229</td>\n",
       "      <td>0.920338</td>\n",
       "      <td>0.928573</td>\n",
       "      <td>0.661943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46327</th>\n",
       "      <td>46327</td>\n",
       "      <td>0.062091</td>\n",
       "      <td>0.157472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739967</td>\n",
       "      <td>0.436768</td>\n",
       "      <td>0.390147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424773</td>\n",
       "      <td>0.448743</td>\n",
       "      <td>0.467676</td>\n",
       "      <td>0.224073</td>\n",
       "      <td>0.919485</td>\n",
       "      <td>0.489150</td>\n",
       "      <td>0.112152</td>\n",
       "      <td>0.070785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46328</th>\n",
       "      <td>46328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.543937</td>\n",
       "      <td>0.791482</td>\n",
       "      <td>0.214576</td>\n",
       "      <td>0.721250</td>\n",
       "      <td>0.214195</td>\n",
       "      <td>0.519369</td>\n",
       "      <td>0.569935</td>\n",
       "      <td>0.960954</td>\n",
       "      <td>0.417863</td>\n",
       "      <td>0.496947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.098834</td>\n",
       "      <td>0.193188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46329</th>\n",
       "      <td>46329</td>\n",
       "      <td>0.390703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608238</td>\n",
       "      <td>0.075430</td>\n",
       "      <td>0.558741</td>\n",
       "      <td>0.310884</td>\n",
       "      <td>0.317546</td>\n",
       "      <td>0.972612</td>\n",
       "      <td>0.187740</td>\n",
       "      <td>0.406213</td>\n",
       "      <td>0.111873</td>\n",
       "      <td>0.069026</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.437820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46330</th>\n",
       "      <td>46330</td>\n",
       "      <td>0.885707</td>\n",
       "      <td>0.044853</td>\n",
       "      <td>0.369273</td>\n",
       "      <td>0.725043</td>\n",
       "      <td>0.563176</td>\n",
       "      <td>0.103416</td>\n",
       "      <td>0.445261</td>\n",
       "      <td>0.927044</td>\n",
       "      <td>0.561305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439365</td>\n",
       "      <td>0.863669</td>\n",
       "      <td>0.277340</td>\n",
       "      <td>0.582923</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46331</th>\n",
       "      <td>46331</td>\n",
       "      <td>0.055595</td>\n",
       "      <td>0.735657</td>\n",
       "      <td>0.438260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.927818</td>\n",
       "      <td>0.665112</td>\n",
       "      <td>0.696661</td>\n",
       "      <td>0.110833</td>\n",
       "      <td>0.389636</td>\n",
       "      <td>0.105947</td>\n",
       "      <td>0.361875</td>\n",
       "      <td>0.586105</td>\n",
       "      <td>0.684263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46332 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index        p1        p2        p3        p4        p5        p6  \\\n",
       "0          0  0.965313  0.204751  0.035420  1.000000  0.159142  0.000000   \n",
       "1          1  0.556415  0.373567  0.358529  0.000000  0.270856  0.229947   \n",
       "2          2  0.752334  1.000000  0.607291  0.002380  0.338792  0.193911   \n",
       "3          3  0.494238  0.235841  0.402977  0.228699  1.000000  0.438163   \n",
       "4          4  0.054635  1.000000  0.309762  0.682028  0.552010  0.589548   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "46327  46327  0.062091  0.157472  1.000000  0.739967  0.436768  0.390147   \n",
       "46328  46328  0.000000  0.543937  0.791482  0.214576  0.721250  0.214195   \n",
       "46329  46329  0.390703  0.000000  1.000000  0.608238  0.075430  0.558741   \n",
       "46330  46330  0.885707  0.044853  0.369273  0.725043  0.563176  0.103416   \n",
       "46331  46331  0.055595  0.735657  0.438260  0.000000  0.927818  0.665112   \n",
       "\n",
       "             p7        p8        p9       p10       p11       p12       p13  \\\n",
       "0      0.124784  0.151610  0.136660  0.193609  0.398131  0.265642  0.128594   \n",
       "1      0.308473  0.762070  1.000000  0.600530  0.113563  0.383396  0.488201   \n",
       "2      0.555606  0.072123  0.530488  0.739669  0.158514  0.474470  0.496779   \n",
       "3      0.311420  0.381508  0.000000  0.377915  0.207145  0.268098  0.249367   \n",
       "4      0.633458  0.907023  0.952393  0.000000  0.071843  0.415229  0.920338   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "46327  0.000000  0.424773  0.448743  0.467676  0.224073  0.919485  0.489150   \n",
       "46328  0.519369  0.569935  0.960954  0.417863  0.496947  1.000000  0.314287   \n",
       "46329  0.310884  0.317546  0.972612  0.187740  0.406213  0.111873  0.069026   \n",
       "46330  0.445261  0.927044  0.561305  0.000000  0.439365  0.863669  0.277340   \n",
       "46331  0.696661  0.110833  0.389636  0.105947  0.361875  0.586105  0.684263   \n",
       "\n",
       "            p14       p15  \n",
       "0      0.372625  0.641762  \n",
       "1      0.440063  0.491090  \n",
       "2      0.268669  0.000000  \n",
       "3      0.330327  0.658996  \n",
       "4      0.928573  0.661943  \n",
       "...         ...       ...  \n",
       "46327  0.112152  0.070785  \n",
       "46328  0.098834  0.193188  \n",
       "46329  0.454600  0.437820  \n",
       "46330  0.582923  1.000000  \n",
       "46331  1.000000  0.197799  \n",
       "\n",
       "[46332 rows x 16 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_to_submit = pd.DataFrame(y_preds).T\n",
    "# results_to_submit.columns = [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\", \"p9\", \"p10\", \"p11\", \"p12\", \"p13\", \"p14\", \"p15\"]\n",
    "# results_to_submit\n",
    "\n",
    "results_to_submit.to_csv(\n",
    "  'results.csv',\n",
    "  header=[\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\", \"p9\", \"p10\", \"p11\", \"p12\", \"p13\", \"p14\", \"p15\"],\n",
    "  index_label='index'\n",
    "  )\n",
    "pd.read_csv(RESULT_CSV, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定義需要的log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import BaseConfig\n",
    "\n",
    "num_epochs = BaseConfig.num_epochs\n",
    "# Number of batchs to show loss\n",
    "num_batches_show_loss = BaseConfig.num_batches_show_loss\n",
    "# Number of batchs to check metrics on validation dataset\n",
    "num_batches_validate = BaseConfig.num_batches_validate\n",
    "batch_size = BaseConfig.batch_size\n",
    "learning_rate = BaseConfig.learning_rate\n",
    "# Number of workers for data loading\n",
    "num_workers = BaseConfig.num_workers\n",
    "# Number of sampled click history for each user\n",
    "num_clicked_news_a_user = BaseConfig.num_clicked_news_a_user\n",
    "num_words_title = BaseConfig.num_words_title\n",
    "num_words_abstract = BaseConfig.num_words_abstract\n",
    "word_freq_threshold = BaseConfig.word_freq_threshold\n",
    "entity_freq_threshold = BaseConfig.entity_freq_threshold\n",
    "entity_confidence_threshold = BaseConfig.entity_confidence_threshold\n",
    "# K\n",
    "negative_sampling_ratio = BaseConfig.negative_sampling_ratio\n",
    "dropout_probability = BaseConfig.dropout_probability\n",
    "# Modify the following by the output of `src/dataprocess.py`\n",
    "num_words = BaseConfig.num_words\n",
    "num_categories = BaseConfig.num_categories\n",
    "num_entities = BaseConfig.num_entities\n",
    "num_users = BaseConfig.num_users\n",
    "word_embedding_dim = BaseConfig.word_embedding_dim\n",
    "category_embedding_dim = BaseConfig.category_embedding_dim\n",
    "# Modify the following only if you use another dataset\n",
    "entity_embedding_dim = BaseConfig.entity_embedding_dim\n",
    "# For additive attention\n",
    "query_vector_dim = BaseConfig.query_vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle competitions submit -c 2023-datamining-final-project -f results.csv -m \"[TANR] AUC: 0.7373, MRR: 0.4098, nDCG@5: 0.4903, nDCG@10: 0.5820  *EXTRA: [num_epochs=5, batch_size=256, learning_rate=0.0001, num_clicked_news_a_user=50, num_words_title=20, num_words_abstract=50, word_freq_threshold=1, entity_freq_threshold=2, entity_confidence_threshold=0.5, negative_sampling_ratio=2, dropout_probability=0.2, ].\"\n"
     ]
    }
   ],
   "source": [
    "EXTRA_MSG: str = ('' + \\\n",
    "  # f'SMOTE+RANDOM stacking ' + \\\n",
    "  f'{num_epochs=}, '\n",
    "  f'{batch_size=}, '\n",
    "  f'{learning_rate=}, '\n",
    "  f'{num_clicked_news_a_user=}, '\n",
    "  f'{num_words_title=}, '\n",
    "  f'{num_words_abstract=}, '\n",
    "  f'{word_freq_threshold=}, '\n",
    "  f'{entity_freq_threshold=}, '\n",
    "  f'{entity_confidence_threshold=}, '\n",
    "  f'{negative_sampling_ratio=}, '\n",
    "  f'{dropout_probability=}, '\n",
    "  # f'take away age>=90 from training data ' + \\\n",
    "  # f'ratio=(8, 2) ' + \\\n",
    "  # f'with normalization ({norm_mode=}) ' + \\\n",
    "  # f'Logistic Regression!' + \\\n",
    "  '')\n",
    "\n",
    "# if REMOVE_MISMATCH:\n",
    "#   EXTRA_MSG += f' | {REMOVE_MISMATCH=}, '\n",
    "# if REFINE_CAPITAL_DIFF:\n",
    "#   EXTRA_MSG += f' | {REFINE_CAPITAL_DIFF=}, '\n",
    "# if REFINE_AGE:\n",
    "#   EXTRA_MSG += f' | {REFINE_AGE=}, '\n",
    "# if REFINE_HPWEEK:\n",
    "#   EXTRA_MSG += f' | {REFINE_HPWEEK=}, '\n",
    "# if REFINE_RACE:\n",
    "#   EXTRA_MSG += f' | {REFINE_RACE=}, '\n",
    "\n",
    "log = (\n",
    "  f\"kaggle competitions submit -c 2023-datamining-final-project -f {RESULT_CSV} -m \"\n",
    "  # f'''\"Features: {best_config['feature']}. INFO: '''\n",
    "  f'''\"[{model_name}] AUC: {auc:.4f}, MRR: {mrr:.4f}, nDCG@5: {ndcg5:.4f}, nDCG@10: {ndcg10:.4f}''' \n",
    "  # [Acc={acc:.4f}, iteration={best_config['iteration']}, lr={best_config['lr']:.6f}, {l2_lambda=:.3f}] \n",
    "  f'''  *EXTRA: [{EXTRA_MSG}].\"'''\n",
    ")\n",
    "print(log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submmit to the Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.7M/11.7M [00:04<00:00, 2.82MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to 2023 Data Mining Final Project"
     ]
    }
   ],
   "source": [
    "# For safty.\n",
    "import os\n",
    "raise KeyError('Are you sure you want to submit the result?')\n",
    "_ = os.system(log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
